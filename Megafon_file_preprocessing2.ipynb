{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 9.97 s, total: 1min 42s\n",
      "Wall time: 1min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Unnamed: 0       id    buy_time          0           1           2  \\\n0           0  2013026  1531688400  18.910029   46.980888    4.969214   \n1           1  2014722  1539550800  36.690029  152.400888  448.069214   \n2           2  2015199  1545598800 -67.019971  157.050888  -63.180786   \n3           3  2021765  1534107600   7.010029  150.200888   -6.930786   \n4           4  2027465  1533502800 -90.439971  134.220888 -104.380786   \n\n            3           4          5         6  ...          243          244  \\\n0   -1.386798    3.791754  -14.01179 -16.08618  ...  -977.373846  -613.770792   \n1  563.833202  463.841754  568.99821 -16.08618  ...  -891.373846  -544.770792   \n2  178.103202  -68.598246  156.99821   3.51382  ...  -977.373846  -613.770792   \n3  216.213202   76.621754  351.84821 -16.08618  ...  -973.373846  -613.770792   \n4  153.643202 -109.798246  132.53821 -16.08618  ...  1643.626154  2007.229208   \n\n          245        246           247        248       249        250  \\\n0  -25.996269 -37.630448   -301.747724 -25.832889 -0.694428 -12.175933   \n1  -20.996269  48.369552     80.252276 -13.832889 -0.694428  -1.175933   \n2  -12.996269 -37.630448  10829.252276 -25.832889 -0.694428 -12.175933   \n3  -23.996269 -37.630448   -205.747724 -24.832889 -0.694428 -11.175933   \n4  206.003731 -21.630448   6667.252276  92.167111 -0.694428  49.824067   \n\n        251  252  \n0  -0.45614  0.0  \n1  -0.45614  0.0  \n2  -0.45614  0.0  \n3  -0.45614  1.0  \n4  47.54386  0.0  \n\n[5 rows x 256 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>buy_time</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>243</th>\n      <th>244</th>\n      <th>245</th>\n      <th>246</th>\n      <th>247</th>\n      <th>248</th>\n      <th>249</th>\n      <th>250</th>\n      <th>251</th>\n      <th>252</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2013026</td>\n      <td>1531688400</td>\n      <td>18.910029</td>\n      <td>46.980888</td>\n      <td>4.969214</td>\n      <td>-1.386798</td>\n      <td>3.791754</td>\n      <td>-14.01179</td>\n      <td>-16.08618</td>\n      <td>...</td>\n      <td>-977.373846</td>\n      <td>-613.770792</td>\n      <td>-25.996269</td>\n      <td>-37.630448</td>\n      <td>-301.747724</td>\n      <td>-25.832889</td>\n      <td>-0.694428</td>\n      <td>-12.175933</td>\n      <td>-0.45614</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2014722</td>\n      <td>1539550800</td>\n      <td>36.690029</td>\n      <td>152.400888</td>\n      <td>448.069214</td>\n      <td>563.833202</td>\n      <td>463.841754</td>\n      <td>568.99821</td>\n      <td>-16.08618</td>\n      <td>...</td>\n      <td>-891.373846</td>\n      <td>-544.770792</td>\n      <td>-20.996269</td>\n      <td>48.369552</td>\n      <td>80.252276</td>\n      <td>-13.832889</td>\n      <td>-0.694428</td>\n      <td>-1.175933</td>\n      <td>-0.45614</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2015199</td>\n      <td>1545598800</td>\n      <td>-67.019971</td>\n      <td>157.050888</td>\n      <td>-63.180786</td>\n      <td>178.103202</td>\n      <td>-68.598246</td>\n      <td>156.99821</td>\n      <td>3.51382</td>\n      <td>...</td>\n      <td>-977.373846</td>\n      <td>-613.770792</td>\n      <td>-12.996269</td>\n      <td>-37.630448</td>\n      <td>10829.252276</td>\n      <td>-25.832889</td>\n      <td>-0.694428</td>\n      <td>-12.175933</td>\n      <td>-0.45614</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2021765</td>\n      <td>1534107600</td>\n      <td>7.010029</td>\n      <td>150.200888</td>\n      <td>-6.930786</td>\n      <td>216.213202</td>\n      <td>76.621754</td>\n      <td>351.84821</td>\n      <td>-16.08618</td>\n      <td>...</td>\n      <td>-973.373846</td>\n      <td>-613.770792</td>\n      <td>-23.996269</td>\n      <td>-37.630448</td>\n      <td>-205.747724</td>\n      <td>-24.832889</td>\n      <td>-0.694428</td>\n      <td>-11.175933</td>\n      <td>-0.45614</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2027465</td>\n      <td>1533502800</td>\n      <td>-90.439971</td>\n      <td>134.220888</td>\n      <td>-104.380786</td>\n      <td>153.643202</td>\n      <td>-109.798246</td>\n      <td>132.53821</td>\n      <td>-16.08618</td>\n      <td>...</td>\n      <td>1643.626154</td>\n      <td>2007.229208</td>\n      <td>206.003731</td>\n      <td>-21.630448</td>\n      <td>6667.252276</td>\n      <td>92.167111</td>\n      <td>-0.694428</td>\n      <td>49.824067</td>\n      <td>47.54386</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 256 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "features = pd.read_csv('features.csv', sep='\\t')\n",
    "train = pd.read_csv('data_train.csv')\n",
    "test = pd.read_csv('data_test.csv')\n",
    "features.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.56 s, sys: 21.6 s, total: 27.1 s\n",
      "Wall time: 1min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Unnamed: 0_x  id  vas_id    buy_time  target  Unnamed: 0_y          0  \\\n0           116   2     2.0  1545598800     0.0     2966746.0 -96.799971   \n1           213   4     1.0  1533502800     0.0           NaN        NaN   \n2           499  15     1.0  1534107600     0.0           NaN        NaN   \n3           513  16     2.0  1540760400     0.0           NaN        NaN   \n4           904  29     1.0  1533502800     0.0     1792640.0 -52.309971   \n\n            1           2           3  ...          243          244  \\\n0  229.530888 -110.740786  305.723202  ...  2300.626154  1492.229208   \n1         NaN         NaN         NaN  ...          NaN          NaN   \n2         NaN         NaN         NaN  ...          NaN          NaN   \n3         NaN         NaN         NaN  ...          NaN          NaN   \n4 -225.139112  -66.250786 -258.246798  ...  -977.373846  -613.770792   \n\n         245        246         247        248       249        250      251  \\\n0 -21.996269 -35.630448  368.252276  11.167111  7.305572 -12.175933 -0.45614   \n1        NaN        NaN         NaN        NaN       NaN        NaN      NaN   \n2        NaN        NaN         NaN        NaN       NaN        NaN      NaN   \n3        NaN        NaN         NaN        NaN       NaN        NaN      NaN   \n4 -25.996269 -37.630448 -304.747724 -25.832889 -0.694428 -12.175933 -0.45614   \n\n   252  \n0  0.0  \n1  NaN  \n2  NaN  \n3  NaN  \n4  0.0  \n\n[5 rows x 259 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0_x</th>\n      <th>id</th>\n      <th>vas_id</th>\n      <th>buy_time</th>\n      <th>target</th>\n      <th>Unnamed: 0_y</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>...</th>\n      <th>243</th>\n      <th>244</th>\n      <th>245</th>\n      <th>246</th>\n      <th>247</th>\n      <th>248</th>\n      <th>249</th>\n      <th>250</th>\n      <th>251</th>\n      <th>252</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>116</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>1545598800</td>\n      <td>0.0</td>\n      <td>2966746.0</td>\n      <td>-96.799971</td>\n      <td>229.530888</td>\n      <td>-110.740786</td>\n      <td>305.723202</td>\n      <td>...</td>\n      <td>2300.626154</td>\n      <td>1492.229208</td>\n      <td>-21.996269</td>\n      <td>-35.630448</td>\n      <td>368.252276</td>\n      <td>11.167111</td>\n      <td>7.305572</td>\n      <td>-12.175933</td>\n      <td>-0.45614</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>213</td>\n      <td>4</td>\n      <td>1.0</td>\n      <td>1533502800</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>499</td>\n      <td>15</td>\n      <td>1.0</td>\n      <td>1534107600</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>513</td>\n      <td>16</td>\n      <td>2.0</td>\n      <td>1540760400</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>904</td>\n      <td>29</td>\n      <td>1.0</td>\n      <td>1533502800</td>\n      <td>0.0</td>\n      <td>1792640.0</td>\n      <td>-52.309971</td>\n      <td>-225.139112</td>\n      <td>-66.250786</td>\n      <td>-258.246798</td>\n      <td>...</td>\n      <td>-977.373846</td>\n      <td>-613.770792</td>\n      <td>-25.996269</td>\n      <td>-37.630448</td>\n      <td>-304.747724</td>\n      <td>-25.832889</td>\n      <td>-0.694428</td>\n      <td>-12.175933</td>\n      <td>-0.45614</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 259 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.merge_asof(train.sort_values('id'),\n",
    "                         features.sort_values('id'),\n",
    "                         on='id', by='buy_time', direction='backward' )\n",
    "\n",
    "test_df = pd.merge_asof(test.sort_values('id'),\n",
    "                        features.sort_values('id'),\n",
    "                        on='id', by='buy_time', direction='backward' )\n",
    "\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Unnamed: 0_x    0\nid              0\nvas_id          0\nbuy_time        0\ntarget          0\n               ..\n248             4\n249             4\n250             4\n251             4\n252             4\nLength: 259, dtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.loc[train_df['0'].notnull()]\n",
    "train_df['0'].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   id  vas_id    buy_time          0           1           2           3  \\\n0  55     2.0  1547413200 -27.139971 -207.729112  -10.930786 -165.866798   \n1  64     4.0  1548018000 -96.799971 -407.009112 -110.740786 -459.616798   \n\n            4          5         6  ...         243         244        245  \\\n0  -16.348246 -186.97179 -16.08618  ... -977.373846 -613.770792 -25.996269   \n1 -116.158246 -480.72179 -16.08618  ... -977.373846 -613.770792 -25.996269   \n\n         246         247        248       249        250      251  252  \n0  -9.630448 -216.747724 -25.832889 -0.694428 -12.175933 -0.45614  0.0  \n1 -37.630448 -306.747724 -25.832889 -0.694428 -12.175933 -0.45614  0.0  \n\n[2 rows x 256 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>vas_id</th>\n      <th>buy_time</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>243</th>\n      <th>244</th>\n      <th>245</th>\n      <th>246</th>\n      <th>247</th>\n      <th>248</th>\n      <th>249</th>\n      <th>250</th>\n      <th>251</th>\n      <th>252</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55</td>\n      <td>2.0</td>\n      <td>1547413200</td>\n      <td>-27.139971</td>\n      <td>-207.729112</td>\n      <td>-10.930786</td>\n      <td>-165.866798</td>\n      <td>-16.348246</td>\n      <td>-186.97179</td>\n      <td>-16.08618</td>\n      <td>...</td>\n      <td>-977.373846</td>\n      <td>-613.770792</td>\n      <td>-25.996269</td>\n      <td>-9.630448</td>\n      <td>-216.747724</td>\n      <td>-25.832889</td>\n      <td>-0.694428</td>\n      <td>-12.175933</td>\n      <td>-0.45614</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>64</td>\n      <td>4.0</td>\n      <td>1548018000</td>\n      <td>-96.799971</td>\n      <td>-407.009112</td>\n      <td>-110.740786</td>\n      <td>-459.616798</td>\n      <td>-116.158246</td>\n      <td>-480.72179</td>\n      <td>-16.08618</td>\n      <td>...</td>\n      <td>-977.373846</td>\n      <td>-613.770792</td>\n      <td>-25.996269</td>\n      <td>-37.630448</td>\n      <td>-306.747724</td>\n      <td>-25.832889</td>\n      <td>-0.694428</td>\n      <td>-12.175933</td>\n      <td>-0.45614</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows × 256 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y'], inplace=True)\n",
    "test_df.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y'], inplace=True)\n",
    "test_df.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(831649, 257)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Уменьшение размеров датасета\n",
    "Уменьшим количество признаков с помощью определения коллинеарных признаков, удалим константные признаки, а также уменьшим объем занимаемой памяти датасета с помощью функции, которая преобразовывает типы данных."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1008x576 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_nunique = train_df.apply(lambda x: x.nunique(dropna=False))\n",
    "\n",
    "plt.title(\"Распределение уникальных значений признаков\");\n",
    "X_nunique.hist(bins=100, figsize=(14, 8));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего признаков (из них было удалено 4): 253\n",
      "Константные признаки: 5\n",
      "Бинарные признаки: 0\n",
      "Категориальные: 0\n",
      "Вещественные признаки: 248\n"
     ]
    },
    {
     "data": {
      "text/plain": "((831649, 257), (71231, 256))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# всего призков (удалим из них buy_time, id, target, vas_id)\n",
    "all_ = set(X_nunique.index.tolist()[4:])\n",
    "\n",
    "#Константные признаки (принимают только одно значение - их можно удалить)\n",
    "const = set(X_nunique[X_nunique == 1].index.tolist())\n",
    "\n",
    "# Числовые признаки\n",
    "numeric = (train_df.fillna(0).astype(int).sum() - train_df.fillna(0).sum()).abs()\n",
    "numeric = set(numeric[numeric > 0].index.tolist())\n",
    "other = all_ - (numeric | const)\n",
    "\n",
    "# Бинарные признаки\n",
    "binary = set(train_df.loc[:, other].columns[(\n",
    "        (train_df.loc[:, other].max() == 1) &\n",
    "        (train_df.loc[:, other].min() == 0) &\n",
    "        (train_df.loc[:, other].isnull().sum() == 0))])\n",
    "\n",
    "# Категориальные признаки\n",
    "categorical = set(X_nunique.loc[other][X_nunique.loc[other] <= 30].index.tolist())\n",
    "numeric_extra = categorical\n",
    "\n",
    "other = other - categorical\n",
    "numeric = numeric | other\n",
    "\n",
    "print('Всего признаков (из них было удалено 4):', train_df.shape[1] - 4)\n",
    "print('Константные признаки:', len(const))\n",
    "print('Бинарные признаки:', len(binary))\n",
    "print('Категориальные:', len(categorical))\n",
    "print('Вещественные признаки:', len(numeric))\n",
    "train_df.shape, test_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "((831649, 252), (71231, 251))"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const = list(const)\n",
    "train_df = train_df.loc[:, ~train_df.columns.isin(const)]\n",
    "test_df = test_df.loc[:, ~test_df.columns.isin(const)]\n",
    "train_df.shape, test_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def remove_collinear_features(df_model, target_var, threshold, verbose):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold and which have the least correlation with the target (dependent) variable. Removing collinear features can help a model\n",
    "        to generalize and improves the interpretability of the model.\n",
    "\n",
    "    Inputs:\n",
    "        df_model: features dataframe\n",
    "        target_var: target (dependent) variable\n",
    "        threshold: features with correlations greater than this value are removed\n",
    "        verbose: set to \"True\" for the log printing\n",
    "\n",
    "    Output:\n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = df_model.drop(target_var, 1).corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "    dropped_feature = \"\"\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                if verbose:\n",
    "                    print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                col_value_corr = df_model[col.values[0]].corr(df_model[target_var])\n",
    "                row_value_corr = df_model[row.values[0]].corr(df_model[target_var])\n",
    "                if verbose:\n",
    "                    print(\"{}: {}\".format(col.values[0], np.round(col_value_corr, 3)))\n",
    "                    print(\"{}: {}\".format(row.values[0], np.round(row_value_corr, 3)))\n",
    "                if col_value_corr < row_value_corr:\n",
    "                    drop_cols.append(col.values[0])\n",
    "                    dropped_feature = \"dropped: \" + col.values[0]\n",
    "                else:\n",
    "                    drop_cols.append(row.values[0])\n",
    "                    dropped_feature = \"dropped: \" + row.values[0]\n",
    "                if verbose:\n",
    "                    print(dropped_feature)\n",
    "                    print(\"-\"*80)\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    df_model = df_model.drop(columns=drops)\n",
    "\n",
    "    print(\"dropped columns: \")\n",
    "    print(list(drops))\n",
    "    print(\"-\"*80)\n",
    "    print(\"used columns: \")\n",
    "    print(df_model.columns.tolist())\n",
    "\n",
    "    return df_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/6yjf0_gx61g9wqrz0b0sxrxc0000gn/T/ipykernel_72577/2044375005.py:19: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  corr_matrix = df_model.drop(target_var, 1).corr()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 | 0 | 0.95\n",
      "2: -0.004\n",
      "0: -0.003\n",
      "dropped: 2\n",
      "-----------------------------------------------------------------------------\n",
      "3 | 1 | 0.93\n",
      "3: -0.007\n",
      "1: -0.007\n",
      "dropped: 1\n",
      "-----------------------------------------------------------------------------\n",
      "4 | 0 | 0.91\n",
      "4: -0.005\n",
      "0: -0.003\n",
      "dropped: 4\n",
      "-----------------------------------------------------------------------------\n",
      "4 | 2 | 0.95\n",
      "4: -0.005\n",
      "2: -0.004\n",
      "dropped: 4\n",
      "-----------------------------------------------------------------------------\n",
      "5 | 1 | 0.88\n",
      "5: -0.01\n",
      "1: -0.007\n",
      "dropped: 5\n",
      "-----------------------------------------------------------------------------\n",
      "5 | 3 | 0.94\n",
      "5: -0.01\n",
      "3: -0.007\n",
      "dropped: 5\n",
      "-----------------------------------------------------------------------------\n",
      "13 | 0 | 0.77\n",
      "13: 0.001\n",
      "0: -0.003\n",
      "dropped: 0\n",
      "-----------------------------------------------------------------------------\n",
      "14 | 0 | 0.81\n",
      "14: -0.0\n",
      "0: -0.003\n",
      "dropped: 0\n",
      "-----------------------------------------------------------------------------\n",
      "14 | 2 | 0.77\n",
      "14: -0.0\n",
      "2: -0.004\n",
      "dropped: 2\n",
      "-----------------------------------------------------------------------------\n",
      "14 | 13 | 0.96\n",
      "14: -0.0\n",
      "13: 0.001\n",
      "dropped: 14\n",
      "-----------------------------------------------------------------------------\n",
      "29 | 28 | 0.85\n",
      "29: 0.001\n",
      "28: -0.002\n",
      "dropped: 28\n",
      "-----------------------------------------------------------------------------\n",
      "30 | 29 | 0.83\n",
      "30: 0.001\n",
      "29: 0.001\n",
      "dropped: 30\n",
      "-----------------------------------------------------------------------------\n",
      "33 | 32 | 0.92\n",
      "33: 0.0\n",
      "32: 0.0\n",
      "dropped: 33\n",
      "-----------------------------------------------------------------------------\n",
      "35 | 17 | 0.98\n",
      "35: -0.001\n",
      "17: -0.001\n",
      "dropped: 35\n",
      "-----------------------------------------------------------------------------\n",
      "40 | 38 | 0.82\n",
      "40: 0.003\n",
      "38: -0.001\n",
      "dropped: 38\n",
      "-----------------------------------------------------------------------------\n",
      "44 | 43 | 0.86\n",
      "44: -0.001\n",
      "43: -0.0\n",
      "dropped: 44\n",
      "-----------------------------------------------------------------------------\n",
      "45 | 43 | 0.91\n",
      "45: 0.001\n",
      "43: -0.0\n",
      "dropped: 43\n",
      "-----------------------------------------------------------------------------\n",
      "51 | 50 | 0.97\n",
      "51: -0.007\n",
      "50: -0.008\n",
      "dropped: 50\n",
      "-----------------------------------------------------------------------------\n",
      "52 | 50 | 0.8\n",
      "52: -0.009\n",
      "50: -0.008\n",
      "dropped: 52\n",
      "-----------------------------------------------------------------------------\n",
      "52 | 51 | 0.82\n",
      "52: -0.009\n",
      "51: -0.007\n",
      "dropped: 52\n",
      "-----------------------------------------------------------------------------\n",
      "53 | 50 | 0.76\n",
      "53: -0.009\n",
      "50: -0.008\n",
      "dropped: 53\n",
      "-----------------------------------------------------------------------------\n",
      "53 | 51 | 0.76\n",
      "53: -0.009\n",
      "51: -0.007\n",
      "dropped: 53\n",
      "-----------------------------------------------------------------------------\n",
      "55 | 54 | 0.79\n",
      "55: -0.001\n",
      "54: 0.001\n",
      "dropped: 55\n",
      "-----------------------------------------------------------------------------\n",
      "63 | 60 | 0.89\n",
      "63: -0.001\n",
      "60: -0.001\n",
      "dropped: 60\n",
      "-----------------------------------------------------------------------------\n",
      "71 | 10 | 1.0\n",
      "71: -0.003\n",
      "10: -0.003\n",
      "dropped: 71\n",
      "-----------------------------------------------------------------------------\n",
      "72 | 12 | 1.0\n",
      "72: -0.001\n",
      "12: -0.001\n",
      "dropped: 72\n",
      "-----------------------------------------------------------------------------\n",
      "74 | 71 | 0.75\n",
      "74: -0.003\n",
      "71: -0.003\n",
      "dropped: 71\n",
      "-----------------------------------------------------------------------------\n",
      "74 | 73 | 0.77\n",
      "74: -0.003\n",
      "73: -0.003\n",
      "dropped: 73\n",
      "-----------------------------------------------------------------------------\n",
      "76 | 19 | 0.84\n",
      "76: -0.001\n",
      "19: -0.003\n",
      "dropped: 19\n",
      "-----------------------------------------------------------------------------\n",
      "77 | 76 | 0.75\n",
      "77: -0.001\n",
      "76: -0.001\n",
      "dropped: 77\n",
      "-----------------------------------------------------------------------------\n",
      "79 | 78 | 0.99\n",
      "79: -0.002\n",
      "78: -0.002\n",
      "dropped: 79\n",
      "-----------------------------------------------------------------------------\n",
      "87 | 83 | 0.95\n",
      "87: -0.001\n",
      "83: -0.001\n",
      "dropped: 83\n",
      "-----------------------------------------------------------------------------\n",
      "88 | 83 | 0.93\n",
      "88: -0.001\n",
      "83: -0.001\n",
      "dropped: 83\n",
      "-----------------------------------------------------------------------------\n",
      "88 | 87 | 0.75\n",
      "88: -0.001\n",
      "87: -0.001\n",
      "dropped: 87\n",
      "-----------------------------------------------------------------------------\n",
      "92 | 91 | 0.89\n",
      "92: -0.002\n",
      "91: -0.002\n",
      "dropped: 91\n",
      "-----------------------------------------------------------------------------\n",
      "104 | 19 | 0.93\n",
      "104: -0.002\n",
      "19: -0.003\n",
      "dropped: 19\n",
      "-----------------------------------------------------------------------------\n",
      "104 | 76 | 0.91\n",
      "104: -0.002\n",
      "76: -0.001\n",
      "dropped: 104\n",
      "-----------------------------------------------------------------------------\n",
      "108 | 106 | 0.84\n",
      "108: -0.002\n",
      "106: 0.0\n",
      "dropped: 108\n",
      "-----------------------------------------------------------------------------\n",
      "112 | 39 | 1.0\n",
      "112: -0.007\n",
      "39: -0.007\n",
      "dropped: 112\n",
      "-----------------------------------------------------------------------------\n",
      "113 | 38 | 0.82\n",
      "113: 0.003\n",
      "38: -0.001\n",
      "dropped: 38\n",
      "-----------------------------------------------------------------------------\n",
      "113 | 40 | 1.0\n",
      "113: 0.003\n",
      "40: 0.003\n",
      "dropped: 40\n",
      "-----------------------------------------------------------------------------\n",
      "116 | 38 | 1.0\n",
      "116: -0.001\n",
      "38: -0.001\n",
      "dropped: 116\n",
      "-----------------------------------------------------------------------------\n",
      "116 | 40 | 0.82\n",
      "116: -0.001\n",
      "40: 0.003\n",
      "dropped: 116\n",
      "-----------------------------------------------------------------------------\n",
      "116 | 113 | 0.82\n",
      "116: -0.001\n",
      "113: 0.003\n",
      "dropped: 116\n",
      "-----------------------------------------------------------------------------\n",
      "117 | 38 | 0.86\n",
      "117: -0.0\n",
      "38: -0.001\n",
      "dropped: 38\n",
      "-----------------------------------------------------------------------------\n",
      "117 | 116 | 0.86\n",
      "117: -0.0\n",
      "116: -0.001\n",
      "dropped: 116\n",
      "-----------------------------------------------------------------------------\n",
      "121 | 118 | 0.93\n",
      "121: -0.002\n",
      "118: -0.003\n",
      "dropped: 118\n",
      "-----------------------------------------------------------------------------\n",
      "123 | 41 | 1.0\n",
      "123: -0.003\n",
      "41: -0.003\n",
      "dropped: 41\n",
      "-----------------------------------------------------------------------------\n",
      "124 | 42 | 1.0\n",
      "124: 0.002\n",
      "42: 0.002\n",
      "dropped: 124\n",
      "-----------------------------------------------------------------------------\n",
      "125 | 42 | 0.79\n",
      "125: -0.0\n",
      "42: 0.002\n",
      "dropped: 125\n",
      "-----------------------------------------------------------------------------\n",
      "125 | 124 | 0.79\n",
      "125: -0.0\n",
      "124: 0.002\n",
      "dropped: 125\n",
      "-----------------------------------------------------------------------------\n",
      "130 | 111 | 0.77\n",
      "130: -0.002\n",
      "111: -0.003\n",
      "dropped: 111\n",
      "-----------------------------------------------------------------------------\n",
      "136 | 135 | 0.77\n",
      "136: 0.011\n",
      "135: 0.007\n",
      "dropped: 135\n",
      "-----------------------------------------------------------------------------\n",
      "137 | 135 | 0.97\n",
      "137: 0.006\n",
      "135: 0.007\n",
      "dropped: 137\n",
      "-----------------------------------------------------------------------------\n",
      "138 | 135 | 0.77\n",
      "138: 0.011\n",
      "135: 0.007\n",
      "dropped: 135\n",
      "-----------------------------------------------------------------------------\n",
      "138 | 136 | 0.98\n",
      "138: 0.011\n",
      "136: 0.011\n",
      "dropped: 138\n",
      "-----------------------------------------------------------------------------\n",
      "138 | 137 | 0.76\n",
      "138: 0.011\n",
      "137: 0.006\n",
      "dropped: 137\n",
      "-----------------------------------------------------------------------------\n",
      "142 | 101 | 0.94\n",
      "142: 0.0\n",
      "101: 0.0\n",
      "dropped: 101\n",
      "-----------------------------------------------------------------------------\n",
      "144 | 143 | 0.85\n",
      "144: 0.004\n",
      "143: 0.003\n",
      "dropped: 143\n",
      "-----------------------------------------------------------------------------\n",
      "146 | 50 | 0.88\n",
      "146: -0.004\n",
      "50: -0.008\n",
      "dropped: 50\n",
      "-----------------------------------------------------------------------------\n",
      "146 | 51 | 0.85\n",
      "146: -0.004\n",
      "51: -0.007\n",
      "dropped: 51\n",
      "-----------------------------------------------------------------------------\n",
      "147 | 54 | 0.9\n",
      "147: 0.002\n",
      "54: 0.001\n",
      "dropped: 54\n",
      "-----------------------------------------------------------------------------\n",
      "151 | 50 | 0.95\n",
      "151: -0.006\n",
      "50: -0.008\n",
      "dropped: 50\n",
      "-----------------------------------------------------------------------------\n",
      "151 | 51 | 0.92\n",
      "151: -0.006\n",
      "51: -0.007\n",
      "dropped: 51\n",
      "-----------------------------------------------------------------------------\n",
      "151 | 52 | 0.77\n",
      "151: -0.006\n",
      "52: -0.009\n",
      "dropped: 52\n",
      "-----------------------------------------------------------------------------\n",
      "151 | 53 | 0.75\n",
      "151: -0.006\n",
      "53: -0.009\n",
      "dropped: 53\n",
      "-----------------------------------------------------------------------------\n",
      "151 | 146 | 0.92\n",
      "151: -0.006\n",
      "146: -0.004\n",
      "dropped: 151\n",
      "-----------------------------------------------------------------------------\n",
      "162 | 54 | 0.97\n",
      "162: 0.001\n",
      "54: 0.001\n",
      "dropped: 54\n",
      "-----------------------------------------------------------------------------\n",
      "162 | 55 | 0.77\n",
      "162: 0.001\n",
      "55: -0.001\n",
      "dropped: 55\n",
      "-----------------------------------------------------------------------------\n",
      "162 | 147 | 0.92\n",
      "162: 0.001\n",
      "147: 0.002\n",
      "dropped: 162\n",
      "-----------------------------------------------------------------------------\n",
      "166 | 44 | 0.87\n",
      "166: -0.0\n",
      "44: -0.001\n",
      "dropped: 44\n",
      "-----------------------------------------------------------------------------\n",
      "167 | 165 | 0.8\n",
      "167: -0.002\n",
      "165: -0.002\n",
      "dropped: 165\n",
      "-----------------------------------------------------------------------------\n",
      "168 | 43 | 0.79\n",
      "168: 0.002\n",
      "43: -0.0\n",
      "dropped: 43\n",
      "-----------------------------------------------------------------------------\n",
      "168 | 45 | 0.86\n",
      "168: 0.002\n",
      "45: 0.001\n",
      "dropped: 45\n",
      "-----------------------------------------------------------------------------\n",
      "169 | 130 | 0.8\n",
      "169: -0.004\n",
      "130: -0.002\n",
      "dropped: 169\n",
      "-----------------------------------------------------------------------------\n",
      "169 | 165 | 0.87\n",
      "169: -0.004\n",
      "165: -0.002\n",
      "dropped: 169\n",
      "-----------------------------------------------------------------------------\n",
      "170 | 43 | 0.86\n",
      "170: -0.001\n",
      "43: -0.0\n",
      "dropped: 170\n",
      "-----------------------------------------------------------------------------\n",
      "170 | 44 | 1.0\n",
      "170: -0.001\n",
      "44: -0.001\n",
      "dropped: 44\n",
      "-----------------------------------------------------------------------------\n",
      "170 | 166 | 0.87\n",
      "170: -0.001\n",
      "166: -0.0\n",
      "dropped: 170\n",
      "-----------------------------------------------------------------------------\n",
      "171 | 111 | 0.76\n",
      "171: -0.004\n",
      "111: -0.003\n",
      "dropped: 171\n",
      "-----------------------------------------------------------------------------\n",
      "171 | 165 | 0.76\n",
      "171: -0.004\n",
      "165: -0.002\n",
      "dropped: 171\n",
      "-----------------------------------------------------------------------------\n",
      "171 | 169 | 0.83\n",
      "171: -0.004\n",
      "169: -0.004\n",
      "dropped: 171\n",
      "-----------------------------------------------------------------------------\n",
      "172 | 44 | 0.85\n",
      "172: -0.001\n",
      "44: -0.001\n",
      "dropped: 44\n",
      "-----------------------------------------------------------------------------\n",
      "172 | 166 | 0.78\n",
      "172: -0.001\n",
      "166: -0.0\n",
      "dropped: 172\n",
      "-----------------------------------------------------------------------------\n",
      "172 | 170 | 0.85\n",
      "172: -0.001\n",
      "170: -0.001\n",
      "dropped: 170\n",
      "-----------------------------------------------------------------------------\n",
      "174 | 173 | 0.82\n",
      "174: -0.003\n",
      "173: -0.003\n",
      "dropped: 173\n",
      "-----------------------------------------------------------------------------\n",
      "176 | 175 | 0.85\n",
      "176: -0.001\n",
      "175: -0.002\n",
      "dropped: 175\n",
      "-----------------------------------------------------------------------------\n",
      "178 | 177 | 0.8\n",
      "178: -0.002\n",
      "177: -0.002\n",
      "dropped: 177\n",
      "-----------------------------------------------------------------------------\n",
      "180 | 179 | 0.91\n",
      "180: -0.002\n",
      "179: -0.002\n",
      "dropped: 179\n",
      "-----------------------------------------------------------------------------\n",
      "182 | 46 | 0.84\n",
      "182: -0.002\n",
      "46: -0.002\n",
      "dropped: 182\n",
      "-----------------------------------------------------------------------------\n",
      "182 | 181 | 0.77\n",
      "182: -0.002\n",
      "181: -0.004\n",
      "dropped: 181\n",
      "-----------------------------------------------------------------------------\n",
      "183 | 46 | 0.79\n",
      "183: -0.003\n",
      "46: -0.002\n",
      "dropped: 183\n",
      "-----------------------------------------------------------------------------\n",
      "183 | 181 | 0.82\n",
      "183: -0.003\n",
      "181: -0.004\n",
      "dropped: 181\n",
      "-----------------------------------------------------------------------------\n",
      "184 | 46 | 0.94\n",
      "184: -0.001\n",
      "46: -0.002\n",
      "dropped: 46\n",
      "-----------------------------------------------------------------------------\n",
      "184 | 183 | 0.79\n",
      "184: -0.001\n",
      "183: -0.003\n",
      "dropped: 183\n",
      "-----------------------------------------------------------------------------\n",
      "185 | 130 | 0.79\n",
      "185: -0.004\n",
      "130: -0.002\n",
      "dropped: 185\n",
      "-----------------------------------------------------------------------------\n",
      "185 | 167 | 0.88\n",
      "185: -0.004\n",
      "167: -0.002\n",
      "dropped: 185\n",
      "-----------------------------------------------------------------------------\n",
      "185 | 169 | 0.78\n",
      "185: -0.004\n",
      "169: -0.004\n",
      "dropped: 185\n",
      "-----------------------------------------------------------------------------\n",
      "186 | 43 | 0.91\n",
      "186: 0.001\n",
      "43: -0.0\n",
      "dropped: 43\n",
      "-----------------------------------------------------------------------------\n",
      "186 | 45 | 1.0\n",
      "186: 0.001\n",
      "45: 0.001\n",
      "dropped: 186\n",
      "-----------------------------------------------------------------------------\n",
      "186 | 168 | 0.86\n",
      "186: 0.001\n",
      "168: 0.002\n",
      "dropped: 186\n",
      "-----------------------------------------------------------------------------\n",
      "187 | 167 | 0.77\n",
      "187: -0.006\n",
      "167: -0.002\n",
      "dropped: 187\n",
      "-----------------------------------------------------------------------------\n",
      "187 | 185 | 0.84\n",
      "187: -0.006\n",
      "185: -0.004\n",
      "dropped: 187\n",
      "-----------------------------------------------------------------------------\n",
      "188 | 43 | 0.76\n",
      "188: -0.002\n",
      "43: -0.0\n",
      "dropped: 188\n",
      "-----------------------------------------------------------------------------\n",
      "188 | 45 | 0.83\n",
      "188: -0.002\n",
      "45: 0.001\n",
      "dropped: 188\n",
      "-----------------------------------------------------------------------------\n",
      "188 | 168 | 0.75\n",
      "188: -0.002\n",
      "168: 0.002\n",
      "dropped: 188\n",
      "-----------------------------------------------------------------------------\n",
      "188 | 186 | 0.83\n",
      "188: -0.002\n",
      "186: 0.001\n",
      "dropped: 188\n",
      "-----------------------------------------------------------------------------\n",
      "190 | 189 | 0.85\n",
      "190: -0.001\n",
      "189: -0.001\n",
      "dropped: 189\n",
      "-----------------------------------------------------------------------------\n",
      "198 | 192 | 0.9\n",
      "198: -0.021\n",
      "192: -0.024\n",
      "dropped: 192\n",
      "-----------------------------------------------------------------------------\n",
      "205 | 198 | 0.77\n",
      "205: -0.025\n",
      "198: -0.021\n",
      "dropped: 205\n",
      "-----------------------------------------------------------------------------\n",
      "217 | 209 | 1.0\n",
      "217: -0.004\n",
      "209: -0.004\n",
      "dropped: 209\n",
      "-----------------------------------------------------------------------------\n",
      "223 | 0 | 0.77\n",
      "223: 0.002\n",
      "0: -0.003\n",
      "dropped: 0\n",
      "-----------------------------------------------------------------------------\n",
      "223 | 13 | 0.89\n",
      "223: 0.002\n",
      "13: 0.001\n",
      "dropped: 13\n",
      "-----------------------------------------------------------------------------\n",
      "223 | 14 | 0.85\n",
      "223: 0.002\n",
      "14: -0.0\n",
      "dropped: 14\n",
      "-----------------------------------------------------------------------------\n",
      "234 | 230 | 0.95\n",
      "234: 0.014\n",
      "230: 0.013\n",
      "dropped: 230\n",
      "-----------------------------------------------------------------------------\n",
      "240 | 236 | 0.89\n",
      "240: -0.001\n",
      "236: -0.0\n",
      "dropped: 240\n",
      "-----------------------------------------------------------------------------\n",
      "244 | 243 | 0.8\n",
      "244: 0.001\n",
      "243: 0.003\n",
      "dropped: 244\n",
      "-----------------------------------------------------------------------------\n",
      "dropped columns: \n",
      "['125', '111', '50', '151', '79', '41', '183', '87', '186', '162', '192', '0', '40', '205', '181', '45', '51', '165', '53', '72', '73', '104', '170', '188', '91', '1', '138', '169', '13', '137', '230', '240', '179', '46', '187', '30', '35', '143', '177', '60', '189', '44', '185', '116', '77', '71', '5', '33', '2', '54', '172', '101', '38', '4', '52', '14', '244', '135', '55', '171', '112', '182', '83', '209', '124', '108', '118', '19', '173', '43', '175', '28']\n",
      "-----------------------------------------------------------------------------\n",
      "used columns: \n",
      "['id', 'vas_id', 'buy_time', 'target', '3', '6', '7', '8', '9', '10', '11', '12', '15', '16', '17', '18', '20', '21', '22', '23', '24', '25', '26', '27', '29', '31', '32', '34', '36', '37', '39', '42', '47', '48', '49', '56', '57', '58', '59', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '74', '76', '78', '80', '82', '84', '86', '88', '89', '90', '92', '93', '94', '95', '96', '97', '98', '99', '100', '102', '103', '105', '106', '107', '109', '110', '113', '114', '115', '117', '119', '120', '121', '122', '123', '126', '127', '128', '129', '130', '131', '132', '133', '134', '136', '140', '141', '142', '144', '145', '146', '147', '148', '149', '150', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '163', '164', '166', '167', '168', '174', '176', '178', '180', '184', '190', '191', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '204', '206', '207', '208', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '231', '232', '233', '234', '235', '236', '237', '238', '239', '241', '242', '243', '245', '246', '247', '248', '249', '250', '251', '252']\n"
     ]
    }
   ],
   "source": [
    "train_df = remove_collinear_features(train_df, 'target', 0.75, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "['id',\n 'vas_id',\n 'buy_time',\n '3',\n '6',\n '7',\n '8',\n '9',\n '10',\n '11',\n '12',\n '15',\n '16',\n '17',\n '18',\n '20',\n '21',\n '22',\n '23',\n '24',\n '25',\n '26',\n '27',\n '29',\n '31',\n '32',\n '34',\n '36',\n '37',\n '39',\n '42',\n '47',\n '48',\n '49',\n '56',\n '57',\n '58',\n '59',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '68',\n '69',\n '70',\n '74',\n '76',\n '78',\n '80',\n '82',\n '84',\n '86',\n '88',\n '89',\n '90',\n '92',\n '93',\n '94',\n '95',\n '96',\n '97',\n '98',\n '99',\n '100',\n '102',\n '103',\n '105',\n '106',\n '107',\n '109',\n '110',\n '113',\n '114',\n '115',\n '117',\n '119',\n '120',\n '121',\n '122',\n '123',\n '126',\n '127',\n '128',\n '129',\n '130',\n '131',\n '132',\n '133',\n '134',\n '136',\n '140',\n '141',\n '142',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '150',\n '152',\n '153',\n '154',\n '155',\n '156',\n '157',\n '158',\n '159',\n '160',\n '161',\n '163',\n '164',\n '166',\n '167',\n '168',\n '174',\n '176',\n '178',\n '180',\n '184',\n '190',\n '191',\n '193',\n '194',\n '195',\n '196',\n '197',\n '198',\n '199',\n '200',\n '201',\n '202',\n '204',\n '206',\n '207',\n '208',\n '210',\n '211',\n '212',\n '213',\n '214',\n '215',\n '216',\n '217',\n '218',\n '219',\n '220',\n '221',\n '222',\n '223',\n '224',\n '225',\n '226',\n '227',\n '228',\n '229',\n '231',\n '232',\n '233',\n '234',\n '235',\n '236',\n '237',\n '238',\n '239',\n '241',\n '242',\n '243',\n '245',\n '246',\n '247',\n '248',\n '249',\n '250',\n '251',\n '252']"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'target'\n",
    "columns = list(train_df.columns)\n",
    "columns.remove(target)\n",
    "columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "((71231, 179), (831649, 180))"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df[columns]\n",
    "test_df.shape, train_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1148.44 MB\n",
      "Memory usage after optimization is: 577.39 MB\n",
      "Decreased by 49.7%\n",
      "Memory usage of dataframe is 97.82 MB\n",
      "Memory usage after optimization is: 49.18 MB\n",
      "Decreased by 49.7%\n"
     ]
    }
   ],
   "source": [
    "train_df = reduce_mem_usage(train_df)\n",
    "test_df = reduce_mem_usage(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "((71231, 179), (831649, 180))"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape, train_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Сохранение готовых датасетов\n",
    "Сохраним преобразованные датасеты в файлы для дальнейшей работы с ними"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv', index=False, encoding='utf-8')\n",
    "test_df.to_csv('test_df.csv', index=False, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IMAGES_PATH = Path(__file__).parent.joinpath('data')\n",
    "# if not IMAGES_PATH.exists():\n",
    "#     IMAGES_PATH.mkdir()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}